{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter hashtag trends per country for the Russo-Ukraine war.\n",
    "##### https://www.kaggle.com/datasets/bwandowando/ukraine-russian-crisis-twitter-dataset-1-2-m-rows?resource=download"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: ensure we can Read/Write data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core dependencies\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import MapType, StringType\n",
    "from pyspark.sql import functions as F\n",
    "from matplotlib import cm\n",
    "from wordcloud import WordCloud\n",
    "from geopy.geocoders import Nominatim\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import random, re\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['OBJC_DISABLE_INITIALIZE_FORK_SAFETY'] = 'YES'\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "seed = random.seed(42)\n",
    "FS_PORT = 9334\n",
    "PARTITIONS = 600\n",
    "APP_NAME = 'DA381A_Project'\n",
    "PATH = f\"hdfs://localhost:{FS_PORT}/project/*\"\n",
    "CUTOFF_LIM = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init spark session\n",
    "spark = SparkSession.builder.appName(APP_NAME).config(\"spark.driver.memory\", \"16g\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", True) # delta caching\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", True) # adaptive query execution for skewed data\n",
    "spark.conf.set(\"spark.databricks.optimizer.rangeJoin.binSize\", 20) #range optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from Hadoop into a DataFrame\n",
    "df = spark.read.csv(path=f\"{PATH}.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+--------+--------+--------------------+---------+-----------+-------------+-------+--------------+------------+----+--------+--------+-----------+--------------+-----------+\n",
      "|                 _c0|              userid|username|acctdesc|location|           following|followers|totaltweets|usercreatedts|tweetid|tweetcreatedts|retweetcount|text|hashtags|language|coordinates|favorite_count|extractedts|\n",
      "+--------------------+--------------------+--------+--------+--------+--------------------+---------+-----------+-------------+-------+--------------+------------+----+--------+--------+-----------+--------------+-----------+\n",
      "|#Russie #Ukraine ...|[{'text': 'Poutin...|      fr|    null|       0|2022-04-01 07:25:...|     null|       null|         null|   null|          null|        null|null|    null|    null|       null|          null|       null|\n",
      "+--------------------+--------------------+--------+--------+--------+--------------------+---------+-----------+-------------+-------+--------------+------------+----+--------+--------+-----------+--------------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.repartition(PARTITIONS)\n",
    "df.show(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will continue now to preprocess the data to be left with the information required to build our graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " 'userid',\n",
       " 'username',\n",
       " 'acctdesc',\n",
       " 'location',\n",
       " 'following',\n",
       " 'followers',\n",
       " 'totaltweets',\n",
       " 'usercreatedts',\n",
       " 'tweetid',\n",
       " 'tweetcreatedts',\n",
       " 'retweetcount',\n",
       " 'text',\n",
       " 'hashtags',\n",
       " 'language',\n",
       " 'coordinates',\n",
       " 'favorite_count',\n",
       " 'extractedts']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = ('userid', 'hashtags')\n",
    "df_filtered = df.select(*cols_to_keep)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As we are interested in the hashtags, we need to remove all tweets that do not have a hashtag and remove all non-dict hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.filter(F.col('hashtags').isNotNull() & ~F.col('hashtags').rlike(r\"^[^{]+$\")) # source: https://chat.openai.com"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We should now retrieve only the text from the hashtag(s) used in the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|             userid|            hashtags|\n",
      "+-------------------+--------------------+\n",
      "|         1350490724|         [Ukrainian]|\n",
      "|1437995794674638852|[SlavaUkraini, Fr...|\n",
      "|1339977139391741953|[Italy, Ukraine️,...|\n",
      "|          233973865|   [Ukraine, Russia]|\n",
      "| 830065408170455040|   [Ukraine, WarDay]|\n",
      "|          219802693|           [Salvini]|\n",
      "|1496947213276221443|         [Anonymous]|\n",
      "| 922217949318008835|     [Ukraine, Kyiv]|\n",
      "|         1556455656|[disinformazione,...|\n",
      "|1468608612134453248|[Ukraine, Russia,...|\n",
      "|1504182118036951045|[UkraineWar, Tigr...|\n",
      "|          110795440|           [Kherson]|\n",
      "|          848832212|    [Macron, Russie]|\n",
      "|         2967943317|           [Ukraine]|\n",
      "|1503405304574271500|[EU, NATO, Butche...|\n",
      "|            5734902|           [Ukraine]|\n",
      "|          265809326|           [GRAMMYs]|\n",
      "|          305526924|[Russia, Asia, Af...|\n",
      "|          964964988|           [Ukraine]|\n",
      "|          335369016|        [UkraineWar]|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df_filtered\n",
    "df2 = df2.withColumn('hashtags', F.split(F.regexp_replace(\"hashtags\", r\"(['\\{\\[\\]\\}\\s:,]+)(text|indices|[0-9]+|')?(?!^[\\p{L}]+$)\", \" \"), \" \"))\n",
    "df2 = df2.withColumn('hashtags', F.array_remove('hashtags',''))\n",
    "df2 = df2.repartition(PARTITIONS)\n",
    "df2.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get all unique userids, and join their hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              userid|            hashtags|\n",
      "+--------------------+--------------------+\n",
      "|                 #AI|           [Ukraine]|\n",
      "|                #BRI|               [BRI]|\n",
      "|            #Cuisine|            [Russie]|\n",
      "|          #Hezbollah|              [Iran]|\n",
      "|            #Hotwife|[Mariupol, Russia...|\n",
      "|              #LGBTQ|  [StandWithUkraine]|\n",
      "| #Nikolayev or #O...|         [Nikolayev]|\n",
      "|    #President_Putin|            [madmen]|\n",
      "|           #Wotblitz|[OmletArcade, Wor...|\n",
      "| #gravity_propulsion|           [Ukraine]|\n",
      "|            #musique|[Russie, UnfoldTh...|\n",
      "|              #nazis|[Ukraine, BBC, Pr...|\n",
      "| #președintele #P...|         [dictatori]|\n",
      "|            #privacy|        [Nordstream]|\n",
      "|           #sériesTV|   [ukraine, guerre]|\n",
      "|             #trader|[phizer, VaccineS...|\n",
      "|          #worldwar3|[Nuclear, War, WW...|\n",
      "|        #டெரக்சாவின்|         [அமெரிக்கா]|\n",
      "|               (City|             [Trump]|\n",
      "|            0 Tweets|[Putin, Corona, U...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df2.groupBy('userid').agg(F.array_distinct(F.flatten(F.collect_list('hashtags'))).alias('hashtags'))\n",
    "df3.repartition(PARTITIONS)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.filter(F.col('userid').rlike(r'^\\d+$'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+\n",
      "|             userid|     hashtags|\n",
      "+-------------------+-------------+\n",
      "|1000010535423938560|   eurovision|\n",
      "|1000033307969630211|        biden|\n",
      "|1000033307969630211|    jameswebb|\n",
      "|1000039842674364417|crimeanbridge|\n",
      "|1000039842674364417|  fashionista|\n",
      "|1000039842674364417|    braywyatt|\n",
      "|1000048501210959872|     starlink|\n",
      "|1000048501210959872|      ukraine|\n",
      "|1000051190422495238|  vladikavkaz|\n",
      "|1000051190422495238|       russia|\n",
      "|1000051190422495238|     azovstal|\n",
      "|1000051190422495238|     mariupol|\n",
      "|1000059059532378112|         nato|\n",
      "|1000082842364215296|      ukraine|\n",
      "|1000085476097839104| fifaworldcup|\n",
      "|1000085476097839104|      usavwal|\n",
      "|1000089708406476802|   ukrainewar|\n",
      "|1000102489490968576|      ukraine|\n",
      "|1000129804333731840|      ukraine|\n",
      "|1000129804333731840|papafrancesco|\n",
      "+-------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[userid: string, hashtags: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df3.withColumn('hashtags', F.explode('hashtags')).withColumn('hashtags', F.trim(F.lower('hashtags')))\n",
    "df3.show()\n",
    "df3.repartition(PARTITIONS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten list of hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|        hashtags| count|\n",
      "+----------------+------+\n",
      "|         ukraine|837161|\n",
      "|          russia|392753|\n",
      "|           putin|277837|\n",
      "|            nato|215889|\n",
      "|        mariupol|173300|\n",
      "|           biden|170790|\n",
      "|standwithukraine|152878|\n",
      "|         russian|129553|\n",
      "|           bucha|114801|\n",
      "|             usa|104222|\n",
      "|        zelensky| 97234|\n",
      "|       ukrainian| 96546|\n",
      "|ukrainerussiawar| 93305|\n",
      "|      eurovision| 90529|\n",
      "|      ukrainewar| 90358|\n",
      "|           china| 88881|\n",
      "|          taiwan| 87510|\n",
      "|    slavaukraini| 85305|\n",
      "|              eu| 76389|\n",
      "|      stoprussia| 73263|\n",
      "+----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df3.withColumn('hashtags', F.regexp_replace('hashtags', 'e️', 'e'))\n",
    "df3 = df3.groupBy('hashtags').count().where(F.col('count') > CUTOFF_LIM).orderBy(F.desc('count'))\n",
    "df3.repartition(PARTITIONS)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df3.toPandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_country(country_name='World'):\n",
    "    # try:\n",
    "    global df\n",
    "    try:\n",
    "        os.mkdir(f'output/{country_name}')\n",
    "    except IOError:\n",
    "        print(f'{country_name} is already written to. Skipping...')\n",
    "        return\n",
    "    country_df = df\n",
    "    country_name = country_name\n",
    "    for n in range(10, 55, 5):\n",
    "            build_barh(country_df, country_name, title=f'Top {n}', limit=n)\n",
    "    build_choropleth(country_name)\n",
    "    try:\n",
    "        build_worldcloud(country_df, country_name)\n",
    "    except ValueError:\n",
    "        print(f'No hashtags survived cut-off. {country_name} returned empty.')\n",
    "\n",
    "    # except:\n",
    "    #     print(f'top level exception reached for country: {country_name} - is this country present in the .shp?')\n",
    "    #     return\n",
    "\n",
    "def build_choropleth(country_name):\n",
    "    country_name = country_name.strip()\n",
    "    plt.clf()\n",
    "    gdf =  gpd.GeoDataFrame(gpd.read_file('shp/World_Countries__Generalized_.shp'))\n",
    "    if 'Antarctica' in gdf.COUNTRY.values:  # bin Antarctica\n",
    "        gdf = gdf[gdf.COUNTRY != 'Antarctica']\n",
    "    if gdf.empty:\n",
    "        print(f'The gdf object is empty, cannot plot with {country_name}.')\n",
    "        return\n",
    "    ax = gdf.plot(column='COUNTRY', cmap='cividis', figsize=(15, 10))\n",
    "    ax.set_axis_off()\n",
    "    ax.set_aspect(1.2)\n",
    "    plt.subplots_adjust(left=0, right=1, bottom=0, top=1)# Turn off the tick marks and tick labels\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    minx, maxx = gdf.total_bounds[0], gdf.total_bounds[2]\n",
    "    miny, maxy = gdf.total_bounds[1], gdf.total_bounds[3]\n",
    "    ax.set_xlim(minx, maxx)\n",
    "    ax.set_ylim(miny, maxy)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'output/{country_name}/{country_name}.png')\n",
    "    plt.close()\n",
    "\n",
    "def build_worldcloud(df, country_name):\n",
    "    plt.clf()\n",
    "    mask = np.array(Image.open(f'output/{country_name}/{country_name}.png'))\n",
    "    wc = WordCloud(background_color='white', mask=mask, contour_width=1, contour_color='black')\n",
    "    wc.generate_from_frequencies(frequencies=df[['hashtags','count']].set_index('hashtags').to_dict()['count'])\n",
    "    wc.to_file(f'output/{country_name}/{country_name} - wordCloud.png') # write to file\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def build_barh(df, country_name, title, limit: int):\n",
    "    plt.clf()\n",
    "    df = df.head(limit)\n",
    "    title = f'{country_name} - {title}'\n",
    "    x = df['count']\n",
    "    y = df['hashtags']\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    cmap = cm.get_cmap('viridis')\n",
    "    colors = cmap(x/x.max()*0.5)\n",
    "    ax.barh(y=y, width=x, height=0.9, color=colors)\n",
    "    ax.invert_yaxis()   \n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Frequency')\n",
    "    ax.set_ylabel('Hashtags')\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'output/{country_name}/{title}.png')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\MachineLearning\\lib\\site-packages\\ipykernel_launcher.py:74: UserWarning: Glyph 3652 (\\N{THAI CHARACTER SARA AI MAIMALAI}) missing from current font.\n",
      "c:\\Anaconda\\envs\\MachineLearning\\lib\\site-packages\\ipykernel_launcher.py:74: UserWarning: Glyph 3605 (\\N{THAI CHARACTER TO TAO}) missing from current font.\n",
      "c:\\Anaconda\\envs\\MachineLearning\\lib\\site-packages\\ipykernel_launcher.py:74: UserWarning: Glyph 3657 (\\N{THAI CHARACTER MAI THO}) missing from current font.\n",
      "c:\\Anaconda\\envs\\MachineLearning\\lib\\site-packages\\ipykernel_launcher.py:74: UserWarning: Glyph 3627 (\\N{THAI CHARACTER HO HIP}) missing from current font.\n",
      "c:\\Anaconda\\envs\\MachineLearning\\lib\\site-packages\\ipykernel_launcher.py:74: UserWarning: Glyph 3623 (\\N{THAI CHARACTER WO WAEN}) missing from current font.\n",
      "c:\\Anaconda\\envs\\MachineLearning\\lib\\site-packages\\ipykernel_launcher.py:74: UserWarning: Glyph 3633 (\\N{THAI CHARACTER MAI HAN-AKAT}) missing from current font.\n",
      "c:\\Anaconda\\envs\\MachineLearning\\lib\\site-packages\\ipykernel_launcher.py:74: UserWarning: Glyph 3609 (\\N{THAI CHARACTER NO NU}) missing from current font.\n",
      "c:\\Anaconda\\envs\\MachineLearning\\lib\\site-packages\\ipykernel_launcher.py:75: UserWarning: Glyph 3652 (\\N{THAI CHARACTER SARA AI MAIMALAI}) missing from current font.\n",
      "c:\\Anaconda\\envs\\MachineLearning\\lib\\site-packages\\ipykernel_launcher.py:75: UserWarning: Glyph 3605 (\\N{THAI CHARACTER TO TAO}) missing from current font.\n",
      "c:\\Anaconda\\envs\\MachineLearning\\lib\\site-packages\\ipykernel_launcher.py:75: UserWarning: Glyph 3657 (\\N{THAI CHARACTER MAI THO}) missing from current font.\n",
      "c:\\Anaconda\\envs\\MachineLearning\\lib\\site-packages\\ipykernel_launcher.py:75: UserWarning: Glyph 3627 (\\N{THAI CHARACTER HO HIP}) missing from current font.\n",
      "c:\\Anaconda\\envs\\MachineLearning\\lib\\site-packages\\ipykernel_launcher.py:75: UserWarning: Glyph 3623 (\\N{THAI CHARACTER WO WAEN}) missing from current font.\n",
      "c:\\Anaconda\\envs\\MachineLearning\\lib\\site-packages\\ipykernel_launcher.py:75: UserWarning: Glyph 3633 (\\N{THAI CHARACTER MAI HAN-AKAT}) missing from current font.\n",
      "c:\\Anaconda\\envs\\MachineLearning\\lib\\site-packages\\ipykernel_launcher.py:75: UserWarning: Glyph 3609 (\\N{THAI CHARACTER NO NU}) missing from current font.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir('output')\n",
    "except:\n",
    "    print('file already exists, skipping mkdir...', flush=True)\n",
    "finally:\n",
    "    output_country()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5274fdbb792524b009a47716b61fa65b00137c1b5ac92b70874abe2d6632ea42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
